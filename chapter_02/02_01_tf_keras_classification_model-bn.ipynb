{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "sys.version_info(major=3, minor=6, micro=10, releaselevel='final', serial=0)\n",
      "matplotlib 2.2.2\n",
      "numpy 1.14.5\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "tensorflow 2.0.0\n",
      "tensorflow.python.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "# 深度网络+dropout\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实战分类模型 数据读取与展示\n",
    "# https://github.com/Dean0371/Tensorflow2.0/blob/225eb6f56a1df56b101b546a694f4825a8e0429e/Tensorflow2.0_%E8%B0%B7%E6%AD%8C/2_Keras/tf_keras_classification_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 28, 28) (5000,)\n",
      "(55000, 28, 28) (55000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(x_train_all, y_train_all), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# numpy的格式，前5000个是验证集\n",
    "x_valid, x_train = x_train_all[:5000], x_train_all[5000:]\n",
    "y_valid, y_train = y_train_all[:5000], y_train_all[5000:]\n",
    "\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "# x_train.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 0\n"
     ]
    }
   ],
   "source": [
    "print(np.max(x_train), np.min(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = (x - u) / std 数据符合标准正态分布\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(\n",
    "    x_train.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
    "x_valid_scaled = scaler.transform(\n",
    "    x_valid.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)\n",
    "x_test_scaled = scaler.transform(\n",
    "    x_test.astype(np.float32).reshape(-1, 1)).reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.023144 -0.81051385\n"
     ]
    }
   ],
   "source": [
    "print(np.max(x_train_scaled), np.min(x_train_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n或\\nmodel = keras.models.Sequential([\\n    keras.layers.Flatten(input_shape=[28,28]),\\n    keras.layers.Dense(300,activation=\"relu\"),\\n    keras.layers.Dense(100,activation=\"relu\"),\\n    keras.layers.Dense(10,activation=\"softmax\")\\n])\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实战分类模型 模型构建\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,activation=\"relu\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "    \"\"\"\n",
    "    1、先激活，再bn\n",
    "    2、先bn，在激活\n",
    "    \"\"\"\n",
    "    \n",
    "model.add(keras.layers.Dense(10,activation=\"softmax\"))\n",
    "# softmax : x = [x1, x2, x3],将向量变成概率分布\n",
    "# y = [e^x1/sum, e^x2/sum,e^x3/sum]   sum = e^x1+e^x2+e^x3\n",
    "\n",
    "# reason for sparse:y->index, index->one hot->[0,...,1,...0]\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer = \"sgd\",  # 优化方法，若loss太低,可能是算法的问题,换用优化过的梯度下降算法\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "或\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(300,activation=\"relu\"),\n",
    "    keras.layers.Dense(100,activation=\"relu\"),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 279,410\n",
      "Trainable params: 275,410\n",
      "Non-trainable params: 4,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [None, 784] * w + b -> [None, 300] w.shape [784, 300], b = [300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 24s 436us/sample - loss: 1.1145 - accuracy: 0.6027 - val_loss: 0.6836 - val_accuracy: 0.7396\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 16s 298us/sample - loss: 0.7670 - accuracy: 0.7286 - val_loss: 0.5597 - val_accuracy: 0.8012\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 16s 298us/sample - loss: 0.6530 - accuracy: 0.7686 - val_loss: 0.4884 - val_accuracy: 0.8292\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 17s 301us/sample - loss: 0.5958 - accuracy: 0.7917 - val_loss: 0.4444 - val_accuracy: 0.8444\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 17s 301us/sample - loss: 0.5494 - accuracy: 0.8080 - val_loss: 0.4418 - val_accuracy: 0.8452\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 17s 304us/sample - loss: 0.5200 - accuracy: 0.8170 - val_loss: 0.4030 - val_accuracy: 0.8614\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 17s 304us/sample - loss: 0.4965 - accuracy: 0.8260 - val_loss: 0.4173 - val_accuracy: 0.8524\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 17s 307us/sample - loss: 0.4713 - accuracy: 0.8340 - val_loss: 0.4059 - val_accuracy: 0.8658\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 17s 309us/sample - loss: 0.4562 - accuracy: 0.8401 - val_loss: 0.3910 - val_accuracy: 0.8612\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 17s 310us/sample - loss: 0.4411 - accuracy: 0.8445 - val_loss: 0.3868 - val_accuracy: 0.8656\n"
     ]
    }
   ],
   "source": [
    "# Tensorboard, earlystopping, ModelCheckpoint\n",
    "logdir = './callbacks'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "output_model_file = os.path.join(logdir,\"fashion_mnist_model.h5\")\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(logdir),\n",
    "    keras.callbacks.ModelCheckpoint(output_model_file,save_best_only = True),  # 默认保存最近一次训练,True表示保存效果最好的\n",
    "    keras.callbacks.EarlyStopping(patience = 5, min_delta = 1e-3)  # 提前结束 当阈值低于1e-3时记录一次,5次后停止\n",
    "]\n",
    "history = model.fit(x_train_scaled,y_train,epochs=10,\n",
    "                    validation_data=(x_valid_scaled,y_valid),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,3)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# 1.参数众多，训练不充分\n",
    "# 2.梯度消失 -> 链式法则 -> 复合求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
